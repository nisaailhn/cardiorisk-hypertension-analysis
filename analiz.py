import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix

# 1. VERÄ° YÃœKLEME

try:
    df = pd.read_csv("veri.csv")

    # patientid analiz iÃ§in gereksiz, varsa Ã§Ä±kar
    if "patientid" in df.columns:
        df = df.drop(columns=["patientid"])

    df = df.dropna()
    print("âœ… Kalp veri seti baÅŸarÄ±yla yÃ¼klendi.")

except FileNotFoundError:
    print("âŒ veri.csv dosyasÄ± bulunamadÄ±")


# 2. Ã–N Ä°ÅLEME
X = df.drop('target', axis=1)
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 3. MODELLERÄ°N ANALÄ°ZÄ°
modeller = {
    "Lojistik Regresyon": LogisticRegression(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "SVM": SVC(probability=True)
}

sonuclar = []

print("\n" + "="*60)
print(f"{'ALGORÄ°TMA':<20} | {'BAÅARI':<10} | {'AYIRT ETME GÃœCÃœ (AUC)'}")
print("-" * 60)

for isim, model in modeller.items():
    model.fit(X_train_scaled, y_train)
    tahmin = model.predict(X_test_scaled)
    olasilik = model.predict_proba(X_test_scaled)[:, 1]
    
    skor = accuracy_score(y_test, tahmin)
    auc_skor = roc_auc_score(y_test, olasilik)
    
    sonuclar.append({"Model": isim, "Skor": skor})
    print(f"{isim:<20} | %{skor*100:.2f}     | {auc_skor:.3f}")

# 4. REGRESYON ANALÄ°ZÄ° (Ã–zellik Ã–nem SÄ±rasÄ±)
# Hangi faktÃ¶r kalbi daha Ã§ok yoruyor?
rf_model = modeller["Random Forest"]
onem_df = pd.DataFrame({
    'FaktÃ¶r': X.columns,
    'Ã–nem Skoru': rf_model.feature_importances_
}).sort_values(by='Ã–nem Skoru', ascending=False)

# 5. GÃ–RSEL ANALÄ°Z PANELÄ°
plt.figure(figsize=(15, 6))

# Grafik 1: FaktÃ¶rlerin Etkisi
plt.subplot(1, 2, 1)
sns.barplot(x='Ã–nem Skoru', y='FaktÃ¶r', data=onem_df, palette='magma')
plt.title('Hangi FaktÃ¶r Kalp HastalÄ±ÄŸÄ±nÄ± Daha Ã‡ok Tetikliyor?')

# Grafik 2: Hata Matrisi (En iyi model iÃ§in)
plt.subplot(1, 2, 2)
en_iyi_model_ismi = max(sonuclar, key=lambda x: x['Skor'])['Model']
cm = confusion_matrix(y_test, modeller[en_iyi_model_ismi].predict(X_test_scaled))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds')
plt.title(f'En Ä°yi Model ({en_iyi_model_ismi}) Tahmin BaÅŸarÄ±sÄ±')
plt.xlabel('Tahmin Edilen')
plt.ylabel('GerÃ§ek Durum')

plt.tight_layout()
plt.show()

print("\n" + "="*60)
print(f"ğŸ† SONUÃ‡: Bu veri setinde en yÃ¼ksek doÄŸruluÄŸu {en_iyi_model_ismi} saÄŸladÄ±.")
print("="*60)
import joblib

# Basit ve hÄ±zlÄ±: Logistic Regression kaydedelim
best_model = LogisticRegression(max_iter=2000)
best_model.fit(X_train_scaled, y_train)

joblib.dump(best_model, "model.pkl")
joblib.dump(scaler, "scaler.pkl")

# SÃ¼tun sÄ±rasÄ±nÄ± da kaydedelim (site input sÄ±rasÄ± doÄŸru olsun diye)
joblib.dump(list(X.columns), "columns.pkl")

print("âœ… model.pkl, scaler.pkl, columns.pkl kaydedildi.")
# EKLENDÄ°: Feature importance'larÄ± kaydet
# ==============================
# Ã–zellik Ã–nemlerini Kaydet
# ==============================

feature_importance_df = pd.DataFrame({
    "feature": X.columns,
    "importance": rf_model.feature_importances_
}).sort_values(by="importance", ascending=False)

feature_importance_df.to_csv("feature_importance.csv", index=False)

print("âœ… feature_importance.csv kaydedildi")
# ==============================
# VERÄ° SETÄ° ORTALAMALARI (HOCALIK KISIM)
# ==============================

group_stats = df.groupby("target").mean()

group_stats.to_csv("group_stats.csv")

print("âœ… group_stats.csv (veri seti ortalamalarÄ±) kaydedildi")
